{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training notebook for Deep-Augmented MUSIC\n",
        "\n",
        "This is the training notebook for deep augmented MUSIC. Note that this notebook is made to be run on Google Colab and will clone the repository by itself.\n",
        "\n",
        "If you are already in the repository, **skip this cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EqrXI60e05CL",
        "outputId": "7aac36dd-29d7-4964-f18f-cd86bd63f96d"
      },
      "outputs": [],
      "source": [
        "!git clone https://MichelDucartier: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@github.com/MichelDucartier/music_doa.git\n",
        "%cd music_doa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3I_qyVQvyJc"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import yaml\n",
        "from IPython.display import clear_output\n",
        "\n",
        "sys.path.append(\"src\")\n",
        "from src.deep.deep_music import DeepMUSIC, rmspe_loss, predict\n",
        "from src.deep.synthetic_data import load_microphones, create_dataset\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CHECKPOINTS_PATH = \"checkpoints/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dataset from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENiRTyxkMR0M",
        "outputId": "2cbcb61c-f93a-4a9e-9927-51b0ff0d0ca7"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_input, data_output, data_n_sources = create_dataset('train_coherent_dataset', 10000, coherent=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load an existing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "filename = \"data/coherent_dataset.h5\"\n",
        "\n",
        "with h5py.File(filename, \"r\") as f:\n",
        "    print(\"Keys: %s\" % f.keys())\n",
        "    data_input = list(f[list(f.keys())[0]])\n",
        "    data_output = list(f[list(f.keys())[1]])\n",
        "    data_n_sources = list(f[list(f.keys())[2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__AYmawcJ1u"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVNHQycMctDE"
      },
      "source": [
        "Here is the training loop.\n",
        "\n",
        "We first import the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOhCgBEUeDPk",
        "outputId": "a6144fba-a464-45c1-dcdd-340fb4b7f2ad"
      },
      "outputs": [],
      "source": [
        "mics_coords = torch.tensor(load_microphones()).to(device)\n",
        "\n",
        "# Load the model\n",
        "with open(\"conf/deep_music.yaml\") as stream:\n",
        "    try:\n",
        "        conf = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "\n",
        "model = DeepMUSIC(mics_coords, conf)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the training loop. If you want to resume checkpoint from a point, uncomment the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checkpoint_epoch = ...\n",
        "# checkpoint_sample = ... \n",
        "\n",
        "# checkpoint = torch.load(os.path.join(CHECKPOINTS_PATH, f'checkpoint-{checkpoint_epoch}-{checkpoint_sample}.pth'))\n",
        "# start_epoch = checkpoint['epoch']\n",
        "\n",
        "# model.load_state_dict(checkpoint[\"model\"])\n",
        "# model.to(device)\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "5PUcMpDNgD1x",
        "outputId": "bf196a34-807f-4036-f4e5-36d9a796aa42"
      },
      "outputs": [],
      "source": [
        "Path(CHECKPOINTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "criterion = rmspe_loss\n",
        "\n",
        "plt.ion()\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "batch_size = 100\n",
        "plot_update = 100\n",
        "save_step = len(data_input) // 2\n",
        "n_epochs = 10\n",
        "\n",
        "running_loss = []\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for epoch in range(start_epoch, n_epochs):\n",
        "  for i, (audio, doas, n_sources) in enumerate(zip(data_input, data_output, data_n_sources)):\n",
        "      audio = torch.tensor(audio).to(device)\n",
        "      n_sources = int(n_sources[0])\n",
        "      doas = torch.tensor(doas[: n_sources]).to(device)\n",
        "\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      estimated_doas, _ = model(audio, n_sources)\n",
        "\n",
        "      loss = criterion(estimated_doas, doas, n_sources)\n",
        "      loss.backward()\n",
        "\n",
        "      if (i+1) % batch_size == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        # Update plot\n",
        "      if (i+1) % plot_update == 0:\n",
        "        X.append(0 if len(X) == 0 else X[-1]+1)\n",
        "        Y.append(np.mean(running_loss))\n",
        "        plt.plot(X, Y, color='b')\n",
        "        clear_output(wait=True)\n",
        "        display(plt.gcf())\n",
        "        running_loss = []\n",
        "\n",
        "      running_loss.append(loss.item())\n",
        "\n",
        "      if i % save_step == 0:\n",
        "        checkpoint = { \n",
        "          'epoch': epoch,\n",
        "          'model': model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        \n",
        "        torch.save(checkpoint, os.path.join(CHECKPOINTS_PATH, f'checkpoint-{epoch}-{i}.pth'))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdfe7hIBOJOE"
      },
      "outputs": [],
      "source": [
        "Path(\"models/\").mkdir(parents=True, exist_ok=True)\n",
        "torch.save(model.state_dict(), \"models/finetuned_model.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
